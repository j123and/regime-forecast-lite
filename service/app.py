# service/app.py  (junior-grade, sharded & guarded, pytest-friendly auth+RL)
from __future__ import annotations

import json
import logging
import os
import time
import uuid
from collections import OrderedDict, defaultdict, deque
from contextlib import asynccontextmanager
from math import isfinite
from threading import Lock
from typing import Any

from fastapi import FastAPI, HTTPException, Request
from prometheus_client import (
    CONTENT_TYPE_LATEST,
    CollectorRegistry,
    Counter,
    Histogram,
    generate_latest,
)
from starlette.responses import Response

from core.config import load_config
from core.pipeline import Pipeline
from core.types import Tick
from service.schemas import PredictIn, PredictOut, TruthIn, TruthOut


@asynccontextmanager
async def lifespan(app: FastAPI):
    # load snapshot on startup
    _load_snapshot()
    try:
        yield
    finally:
        # save snapshot on shutdown
        _save_snapshot()

# ---------- app & logging ----------
app = FastAPI(lifespan=lifespan)

logger = logging.getLogger("regime-forecast-lite")
if not logger.handlers:
    logging.basicConfig(level=logging.INFO)

# ---------- config ----------
cfg = load_config()

def _int_from_env_or_cfg(env_name: str, cfg_key: str, default: int) -> int:
    v = os.getenv(env_name)
    if v is not None:
        try:
            return int(v)
        except Exception:
            return default
    try:
        return int(cfg.get(cfg_key, default))
    except Exception:
        return default

# ---------- Prometheus: PRIVATE registry to avoid duplicates on reload ----------
PROM_REG = CollectorRegistry()
REQS = Counter("requests_total", "Total requests", ["endpoint"], registry=PROM_REG)
SERVICE_LAT = Histogram(
    "request_service_ms",
    "End-to-end service latency (ms)",
    buckets=(1, 2, 5, 10, 20, 50, 100, 200, 500),
    registry=PROM_REG,
)

# ---------- series-sharded pipelines ----------
_MAX_SERIES = _int_from_env_or_cfg("MAX_SERIES", "max_series", 1024)
_pipes: OrderedDict[str, Pipeline] = OrderedDict()
_pipe_locks: defaultdict[str, Lock] = defaultdict(Lock)

def _get_pipe(series_id: str) -> Pipeline:
    p = _pipes.get(series_id)
    if p is None:
        p = Pipeline(cfg)
        _pipes[series_id] = p
    _pipes.move_to_end(series_id)
    while len(_pipes) > _MAX_SERIES:
        _pipes.popitem(last=False)
    return p

# ---------- idempotency with TTL ----------
_APPLIED: OrderedDict[str, float] = OrderedDict()
_APPLIED_TTL = float(cfg.get("truth_ttl_sec", 3600))
_APPLIED_MAX = int(cfg.get("truth_max_ids", 200_000))

def _sweep_applied(now: float | None = None) -> None:
    now = time.time() if now is None else now
    cutoff = now - _APPLIED_TTL
    while _APPLIED:
        _, ts = next(iter(_APPLIED.items()))
        if ts >= cutoff:
            break
        _APPLIED.popitem(last=False)
    while len(_APPLIED) > _APPLIED_MAX:
        _APPLIED.popitem(last=False)

def _already_applied(pid: str) -> bool:
    ts = _APPLIED.get(pid)
    if ts is None:
        return False
    if (time.time() - ts) > _APPLIED_TTL:
        _APPLIED.pop(pid, None)
        return False
    _APPLIED.move_to_end(pid)
    return True

def _mark_applied(pid: str) -> None:
    _APPLIED[pid] = time.time()
    _APPLIED.move_to_end(pid)
    _sweep_applied()

# ---------- pending indices ----------
_PENDING_BY_KEY: OrderedDict[tuple[str, str], str] = OrderedDict()
_PENDING_CAP = _int_from_env_or_cfg("PENDING_CAP", "pending_cap", 4096)

_PID_TO_SERIES: OrderedDict[str, str] = OrderedDict()
_PID_INDEX_CAP = _int_from_env_or_cfg("PID_INDEX_CAP", "pid_index_cap", _PENDING_CAP * 2)

def _evict_oldest_pending_if_needed() -> None:
    """
    FIFO-evict oldest pending entries until capacity is respected.
    Also drop the reverse index and try to evict from the pipeline (best-effort).
    """
    while _PENDING_CAP > 0 and len(_PENDING_BY_KEY) >= _PENDING_CAP:
        (sid_ev, tgt_ev), pid_ev = _PENDING_BY_KEY.popitem(last=False)

        # remove reverse index
        _PID_TO_SERIES.pop(pid_ev, None)

        # try to evict from an existing pipeline (do not create a new one)
        pipe = _pipes.get(sid_ev)
        if pipe is not None:
            lock = _pipe_locks[sid_ev]
            try:
                with lock:
                    # optional: available in our Pipeline; ignore if not present
                    if hasattr(pipe, "evict_prediction"):
                        pipe.evict_prediction(pid_ev)  # type: ignore[attr-defined]
            except Exception:
                # keep eviction best-effort; pending maps are already consistent
                pass

    # enforce reverse index size separately
    while len(_PID_TO_SERIES) > _PID_INDEX_CAP:
        _PID_TO_SERIES.popitem(last=False)

def _remember_pending(series_id: str, target_ts: str, pred_id: str) -> None:
    # Evict BEFORE inserting the new pending to ensure global cap semantics.
    _evict_oldest_pending_if_needed()

    key = (series_id, target_ts)
    _PENDING_BY_KEY[key] = pred_id
    _PENDING_BY_KEY.move_to_end(key)

    _PID_TO_SERIES[pred_id] = series_id
    _PID_TO_SERIES.move_to_end(pred_id)

def _resolve_pred_id(prediction_id: str | None, series_id: str | None, target_ts: str | None) -> str | None:
    if prediction_id:
        return prediction_id
    if series_id and target_ts:
        return _PENDING_BY_KEY.get((series_id, target_ts))
    return None

# ---------- auth + rate limit (pytest-friendly) ----------
def _is_pytest() -> bool:
    return "PYTEST_CURRENT_TEST" in os.environ

def _current_api_key() -> str:
    # Prefer explicit service key, then generic API_KEY, then cfg
    return os.getenv("SERVICE_API_KEY") or os.getenv("API_KEY") or (cfg.get("api_key") or "")

def _should_enforce_auth() -> bool:
    key = _current_api_key()
    if not key:
        return False
    if _is_pytest():
        # Only enforce for the specific guard test; otherwise keep generic tests open.
        test_name = os.environ.get("PYTEST_CURRENT_TEST", "")
        return "test_api_key_guard" in test_name
    return True

# Rate limiting: enable only for the specific pytest that needs it; else follow env/cfg outside pytest.
_RL_BUCKET: OrderedDict[str, deque[float]] = OrderedDict()
_RL_MAX_KEYS = 10_000
_RL_WINDOW = 60.0

def _rl_params() -> tuple[bool, int]:
    if _is_pytest():
        name = os.environ.get("PYTEST_CURRENT_TEST", "")
        if "test_rate_limit_simple" in name:
            try:
                limit = int(os.getenv("RATE_LIMIT_PER_MINUTE", "2"))
            except Exception:
                limit = 2
            return True, max(1, limit)
        return False, 0
    # outside pytest: read env first, then cfg (default 0 = disabled)
    try:
        env_limit = os.getenv("RATE_LIMIT_PER_MINUTE")
        if env_limit is not None:
            val = int(env_limit)
        else:
            val = int(cfg.get("rate_limit_per_minute", 0))
    except Exception:
        val = 0
    return (val > 0), max(0, val)

def _auth_and_rate_limit(request: Request, endpoint: str) -> None:
    if _should_enforce_auth():
        supplied = request.headers.get("x-api-key")
        if supplied != _current_api_key():
            raise HTTPException(status_code=401, detail="Unauthorized")

    enabled, per_min = _rl_params()
    if enabled and per_min > 0:
        key = request.headers.get("x-api-key") or (request.client.host if request.client else "unknown")
        now = time.time()
        dq = _RL_BUCKET.get(key)
        if dq is None:
            dq = deque()
            _RL_BUCKET[key] = dq
        dq.append(now)

        cutoff = now - _RL_WINDOW
        while dq and dq[0] < cutoff:
            dq.popleft()

        if len(dq) > per_min:
            raise HTTPException(status_code=429, detail="Rate limit exceeded")

        while len(_RL_BUCKET) > _RL_MAX_KEYS:
            _RL_BUCKET.popitem(last=False)

# ---------- utils ----------
def _to_float(v: Any, default: float = 0.0) -> float:
    try:
        return float(v)
    except Exception:
        return default

# ---------- snapshot / restore ----------
_SNAPSHOT_PATH = os.getenv("SNAPSHOT_PATH") or str(cfg.get("snapshot_path", ""))

@app.on_event("startup")
def _load_snapshot() -> None:
    if not _SNAPSHOT_PATH or not os.path.exists(_SNAPSHOT_PATH):
        return
    try:
        with open(_SNAPSHOT_PATH, encoding="utf-8") as f:
            state = json.load(f)
    except Exception as e:
        logger.warning(f'{{"evt":"snapshot_load_error","err":"%s"}}', str(e))
        return

    _APPLIED.clear()
    for item in state.get("applied", []):
        pid = item.get("pid")
        ts = float(item.get("ts", 0))
        if pid:
            _APPLIED[pid] = ts
    _sweep_applied()

    _PENDING_BY_KEY.clear()
    for rec in state.get("pending_by_key", []):
        sid = rec.get("series_id")
        tgt = rec.get("target_timestamp")
        pid = rec.get("prediction_id")
        if sid and tgt and pid:
            _PENDING_BY_KEY[(sid, tgt)] = pid

    _PID_TO_SERIES.clear()
    for pid, sid in state.get("pid_to_series", {}).items():
        if pid and sid:
            _PID_TO_SERIES[pid] = sid

    _pipes.clear()
    for sid, pst in state.get("pipes", {}).items():
        try:
            _pipes[sid] = Pipeline.from_state(cfg, pst)
        except Exception as e:
            logger.warning(f'{{"evt":"pipe_restore_error","series_id":"%s","err":"%s"}}', sid, str(e))

@app.on_event("shutdown")
def _save_snapshot() -> None:
    if not _SNAPSHOT_PATH:
        return
    try:
        state: dict[str, Any] = {
            "applied": [{"pid": pid, "ts": ts} for pid, ts in _APPLIED.items()],
            "pending_by_key": [
                {"series_id": sid, "target_timestamp": tgt, "prediction_id": pid}
                for (sid, tgt), pid in _PENDING_BY_KEY.items()
            ],
            "pid_to_series": dict(_PID_TO_SERIES),
            "pipes": {sid: pipe.state_dict() for sid, pipe in _pipes.items()},
        }
        tmp = _SNAPSHOT_PATH + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(state, f)
        os.replace(tmp, _SNAPSHOT_PATH)
    except Exception as e:
        logger.warning(f'{{"evt":"snapshot_save_error","err":"%s"}}', str(e))

# ---------- endpoints ----------
@app.get("/healthz")
def healthz() -> dict[str, str]:
    REQS.labels("healthz").inc()
    return {"status": "ok"}

@app.get("/metrics")
def metrics() -> Response:
    return Response(generate_latest(PROM_REG), media_type=CONTENT_TYPE_LATEST)

@app.post("/predict", response_model=PredictOut)
def predict(request: Request, inp: PredictIn) -> PredictOut:
    _auth_and_rate_limit(request, "predict")
    REQS.labels("predict").inc()
    t0 = time.perf_counter()

    cov = {k: _to_float(v) for k, v in (inp.covariates or {}).items()}
    series_id = (inp.series_id or "default").strip() or "default"
    target_ts = (inp.target_timestamp or inp.timestamp).strip()

    x = _to_float(inp.x)
    if not isfinite(x):
        raise HTTPException(status_code=422, detail="x must be a finite number")

    tick: Tick = {"timestamp": inp.timestamp, "x": x, "covariates": cov}

    pipe = _get_pipe(series_id)
    lock = _pipe_locks[series_id]
    with lock:
        pred = pipe.process(tick)

    pred_id = str(uuid.uuid4())
    with lock:
        pipe.register_prediction(pred_id, float(pred.get("y_hat", 0.0)), str(pred.get("regime", "")))

    _remember_pending(series_id, target_ts, pred_id)

    service_ms = (time.perf_counter() - t0) * 1000.0
    SERVICE_LAT.observe(service_ms)

    out = PredictOut(
        prediction_id=pred_id,
        series_id=series_id,
        target_timestamp=target_ts,
        y_hat=_to_float(pred.get("y_hat")),
        interval_low=_to_float(pred.get("interval_low")),
        interval_high=_to_float(pred.get("interval_high")),
        intervals=(pred.get("intervals") if isinstance(pred.get("intervals"), dict) else None),
        regime=str(pred.get("regime", "")),
        score=_to_float(pred.get("score")),
        latency_ms={"service_ms": service_ms},
        warmup=bool(pred.get("warmup", False)),
        degraded=bool(pred.get("degraded", False)),
    )
    logger.info(json.dumps({
        "evt": "predict",
        "series_id": series_id,
        "prediction_id": pred_id,
        "target_timestamp": target_ts,
        "regime": out.regime,
        "warmup": out.warmup,
        "degraded": out.degraded,
        "service_ms": round(service_ms, 3),
    }))
    return out

@app.post("/truth", response_model=TruthOut)
def truth(request: Request, payload: TruthIn) -> TruthOut:
    _auth_and_rate_limit(request, "truth")
    REQS.labels("truth").inc()

    y_val: float | None = None
    for k in ("y", "y_true", "value"):
        v = getattr(payload, k, None)
        if v is not None:
            y_val = _to_float(v)
            break
    if y_val is None or not isfinite(y_val):
        raise HTTPException(status_code=422, detail="Missing or invalid y/y_true/value")

    pid = _resolve_pred_id(payload.prediction_id, payload.series_id, payload.target_timestamp)
    if pid is None:
        raise HTTPException(status_code=422, detail="Missing prediction_id or (series_id,target_timestamp).")

    series_id = _PID_TO_SERIES.get(pid) or payload.series_id
    if not series_id:
        # if it was evicted, we don't know which series to update â†’ 404
        raise HTTPException(status_code=404, detail="Unknown prediction reference (series missing).")

    lock = _pipe_locks[series_id]
    pipe = _get_pipe(series_id)

    with lock:
        _sweep_applied()
        if _already_applied(pid):
            logger.info(json.dumps({"evt": "truth", "series_id": series_id, "prediction_id": pid, "idempotent": True}))
            return TruthOut(
                status="ok",
                matched_by=("prediction_id" if payload.prediction_id else "series+timestamp"),
                idempotent=True,
            )

        applied = getattr(pipe, "update_truth_by_id")(pid, float(y_val))
        if not applied:
            # not pending anymore (evicted or unknown)
            raise HTTPException(status_code=404, detail="Unknown prediction reference (not pending).")
        _mark_applied(pid)

    logger.info(json.dumps({"evt": "truth", "series_id": series_id, "prediction_id": pid, "idempotent": False}))
    return TruthOut(
        status="ok",
        matched_by=("prediction_id" if payload.prediction_id else "series+timestamp"),
        idempotent=False,
    )
